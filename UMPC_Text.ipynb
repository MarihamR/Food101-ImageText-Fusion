{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6242131",
   "metadata": {},
   "source": [
    "# UMPC-Food101 - Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "335e9842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home1/ece1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from os import path as osp\n",
    "import os\n",
    "import logging\n",
    "from torch.serialization import save\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import activation\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn,optim\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import BertModel, BertConfig,AutoModel\n",
    "from torch_lr_finder import LRFinder\n",
    "import random\n",
    "\n",
    "from TextModels.Text_main import main\n",
    "from UMPC_Dataset import UMPC_FoodDataset\n",
    "\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import requests\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "\n",
    "\n",
    "a=15\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(a)\n",
    "np.random.seed(a)\n",
    "torch.manual_seed(a)\n",
    "torch.cuda.manual_seed(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a1ce570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_SIZE =101\n",
    "epochs=15\n",
    "batch_size=128\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41ec4cd",
   "metadata": {},
   "source": [
    "## 1. Train/Test Splits and DataLoaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c98be0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=UMPC_FoodDataset(targ_dir=\"./datasets/Food101\" ,phase='train', mode=\"Text_only\")\n",
    "train_loader = DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "test_set=UMPC_FoodDataset(targ_dir=\"./datasets/Food101\" ,phase=\"test\", mode=\"Text_only\")\n",
    "test_loader = DataLoader(\n",
    "    test_set, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "212a2ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_set: 67988,  Test_set: 22716\n",
      "Total no. of batches in trainloader : 532\n",
      "Total no. of batches in testloader : 178\n"
     ]
    }
   ],
   "source": [
    "print (\"Train_set: \"+str(len(train_set))+\",  Test_set: \"+str(len(test_set)))\n",
    "print(f\"Total no. of batches in trainloader : {len(train_loader)}\")\n",
    "print(f\"Total no. of batches in testloader : {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12faa2fa",
   "metadata": {},
   "source": [
    "### Samples of dataset : trainset = tuple (txt_tokens,txt,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c6d098e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([  101, 13675,  7432,  1011,  8962,  6456, 13675,  7432,  1011,  8962,\n",
       "           6207, 11345, 23377, 14014,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       "  'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])},\n",
       " 'Crock-Pot Ladies  Crock-Pot Apple Pie Moonshine',\n",
       " 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "002687cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:Super Bowl Recipes | Whisk Together\n",
      "Class:40-french_fries\n",
      "\n",
      "\n",
      "Text:Persian Baklava Recipe by the.instructor | iFood.tv\n",
      "Class:2-baklava\n",
      "\n",
      "\n",
      "Text:Bibimbap (Mixed rice with vegetables) recipe - Maangchi.com\n",
      "Class:7-bibimbap\n",
      "\n",
      "\n",
      "Text:Top 10 deviled eggs recipes: From curried to truffled | HellaWella\n",
      "Class:30-deviled_eggs\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "randomlist = random.sample(range(0, len(train_set)), 4)\n",
    "for idx in randomlist:\n",
    "    txt_tokens,txt,label=train_set[idx]\n",
    "    print (f\"Text:{txt}\\nClass:{label}-{train_set.idx_to_class[label]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3cbadf",
   "metadata": {},
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70109ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextModel(nn.Module):\n",
    "    def __init__(self, dim_text_repr=768, num_class=101):\n",
    "        super().__init__()\n",
    "        config = BertConfig()\n",
    "        self.textEncoder= BertModel(config).from_pretrained('bert-base-uncased')    \n",
    "        self.linear = nn.Linear(dim_text_repr, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        text = x\n",
    "        hidden_states = self.textEncoder(**text)  # B, T, dim_text_repr\n",
    "        e_i = F.dropout(hidden_states[1]) \n",
    "        return self.linear(e_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8a78e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = TextModel(num_class=OUTPUT_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ebbb21",
   "metadata": {},
   "source": [
    "## 3. Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5efaf9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb056fdc",
   "metadata": {},
   "source": [
    "## 4. Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fbcee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eced9c4",
   "metadata": {},
   "source": [
    "## 5.Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d33d946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep[Train]1/20: 100%|██| 532/532 [03:27<00:00,  2.56it/s, loss=4.081, acc=0.1120]\n",
      "Epoch[Test]1/20: 100%|█| 178/178 [00:28<00:00,  6.17it/s, loss=2.865, acc=0.4207\n",
      "Ep[Train]2/20: 100%|██| 532/532 [03:27<00:00,  2.56it/s, loss=1.943, acc=0.6861]\n",
      "Epoch[Test]2/20: 100%|█| 178/178 [00:28<00:00,  6.16it/s, loss=1.204, acc=0.8156\n",
      "Ep[Train]3/20: 100%|██| 532/532 [03:28<00:00,  2.56it/s, loss=1.080, acc=0.8261]\n",
      "Epoch[Test]3/20: 100%|█| 178/178 [00:28<00:00,  6.17it/s, loss=0.888, acc=0.8359\n",
      "Ep[Train]4/20: 100%|██| 532/532 [03:28<00:00,  2.56it/s, loss=0.883, acc=0.8366]\n",
      "Epoch[Test]4/20: 100%|█| 178/178 [00:28<00:00,  6.14it/s, loss=0.824, acc=0.8406\n",
      "Ep[Train]5/20: 100%|██| 532/532 [03:28<00:00,  2.56it/s, loss=0.811, acc=0.8411]\n",
      "Epoch[Test]5/20: 100%|█| 178/178 [00:28<00:00,  6.16it/s, loss=0.835, acc=0.8379\n",
      "Ep[Train]6/20: 100%|██| 532/532 [03:28<00:00,  2.56it/s, loss=0.765, acc=0.8466]\n",
      "Epoch[Test]6/20: 100%|█| 178/178 [00:28<00:00,  6.15it/s, loss=0.783, acc=0.8443\n",
      "Ep[Train]7/20: 100%|██| 532/532 [03:28<00:00,  2.56it/s, loss=0.735, acc=0.8483]\n",
      "Epoch[Test]7/20: 100%|█| 178/178 [00:28<00:00,  6.15it/s, loss=0.801, acc=0.8426\n",
      "Ep[Train]8/20: 100%|██| 532/532 [03:28<00:00,  2.55it/s, loss=0.709, acc=0.8523]\n",
      "Epoch[Test]8/20: 100%|█| 178/178 [00:29<00:00,  6.14it/s, loss=0.749, acc=0.8465\n",
      "Ep[Train]9/20: 100%|██| 532/532 [03:28<00:00,  2.55it/s, loss=0.687, acc=0.8547]\n",
      "Epoch[Test]9/20: 100%|█| 178/178 [00:29<00:00,  6.13it/s, loss=0.769, acc=0.8460\n",
      "Ep[Train]10/20: 100%|█| 532/532 [03:28<00:00,  2.55it/s, loss=0.664, acc=0.8581]\n",
      "Epoch[Test]10/20: 100%|█| 178/178 [00:28<00:00,  6.14it/s, loss=0.745, acc=0.850\n",
      "Ep[Train]11/20: 100%|█| 532/532 [03:28<00:00,  2.56it/s, loss=0.642, acc=0.8622]\n",
      "Epoch[Test]11/20: 100%|█| 178/178 [00:28<00:00,  6.14it/s, loss=0.746, acc=0.850\n",
      "Ep[Train]12/20: 100%|█| 532/532 [03:28<00:00,  2.56it/s, loss=0.624, acc=0.8647]\n",
      "Epoch[Test]12/20: 100%|█| 178/178 [00:28<00:00,  6.16it/s, loss=0.757, acc=0.849\n",
      "Ep[Train]13/20: 100%|█| 532/532 [03:28<00:00,  2.56it/s, loss=0.604, acc=0.8672]\n",
      "Epoch[Test]13/20: 100%|█| 178/178 [00:28<00:00,  6.15it/s, loss=0.739, acc=0.853\n",
      "Ep[Train]14/20: 100%|█| 532/532 [03:28<00:00,  2.55it/s, loss=0.583, acc=0.8706]\n",
      "Epoch[Test]14/20: 100%|█| 178/178 [00:28<00:00,  6.15it/s, loss=0.742, acc=0.854\n",
      "Ep[Train]15/20: 100%|█| 532/532 [03:28<00:00,  2.55it/s, loss=0.563, acc=0.8741]\n",
      "Epoch[Test]15/20: 100%|█| 178/178 [00:28<00:00,  6.14it/s, loss=0.739, acc=0.855\n",
      "Ep[Train]16/20: 100%|█| 532/532 [03:28<00:00,  2.56it/s, loss=0.546, acc=0.8774]\n",
      "Epoch[Test]16/20: 100%|█| 178/178 [00:28<00:00,  6.14it/s, loss=0.751, acc=0.854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep[Train]17/20: 100%|█| 532/532 [03:28<00:00,  2.55it/s, loss=0.502, acc=0.8849]\n",
      "Epoch[Test]17/20: 100%|█| 178/178 [00:28<00:00,  6.14it/s, loss=0.741, acc=0.857\n",
      "Ep[Train]18/20: 100%|█| 532/532 [03:28<00:00,  2.55it/s, loss=0.492, acc=0.8867]\n",
      "Epoch[Test]18/20: 100%|█| 178/178 [00:28<00:00,  6.15it/s, loss=0.744, acc=0.857\n",
      "Ep[Train]19/20: 100%|█| 532/532 [03:28<00:00,  2.56it/s, loss=0.487, acc=0.8880]\n",
      "Epoch[Test]19/20: 100%|█| 178/178 [00:28<00:00,  6.14it/s, loss=0.744, acc=0.857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep[Train]20/20: 100%|█| 532/532 [03:28<00:00,  2.55it/s, loss=0.482, acc=0.8892]\n",
      "Epoch[Test]20/20: 100%|█| 178/178 [00:28<00:00,  6.16it/s, loss=0.742, acc=0.858\n",
      "Epoch[Test]1/1: 100%|█| 178/178 [00:28<00:00,  6.17it/s, loss=0.742, acc=0.8580]\n"
     ]
    }
   ],
   "source": [
    "main(model,train_loader,test_loader,test_loader,loss_fn,\n",
    "     optimizer,lr=learning_rate ,name=\"BERT_UMPC\",epochs=20,save_weights=True,scheduler_bol=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb60b98",
   "metadata": {},
   "source": [
    "## 6.Load Saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65099206",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TextModel(num_class=OUTPUT_SIZE).to(device)\n",
    "model.load_state_dict(torch.load(\"./Saved_weights/BERT_UMPC_best_weights.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1d6db97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[Test]1/1: 100%|█| 178/178 [00:28<00:00,  6.20it/s, loss=0.741, acc=0.8528]\n"
     ]
    }
   ],
   "source": [
    "main(model,train_loader,test_loader,test_loader,loss_fn,optimizer,lr=learning_rate,Test=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
